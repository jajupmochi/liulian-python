{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"],"fields":{"title":{"boost":1000.0},"text":{"boost":1.0},"tags":{"boost":1000000.0}}},"docs":[{"location":"","title":"LIULIAN","text":"<p>Liquid Intelligence and Unified Logic for Interactive Adaptive Networks</p> <p>\"Where Space and Time Converge in Intelligence\"</p> <p>Welcome to the LIULIAN documentation \u2014 a Research OS for spatiotemporal model experimentation.</p>"},{"location":"#what-is-liulian","title":"What is LIULIAN?","text":"<p>LIULIAN is a Python library that provides a unified, task-driven framework for training, evaluating, and performing inference with time-series, graph, and spatiotemporal models. It is designed for researchers who need reproducible experiment pipelines with minimal boilerplate.</p>"},{"location":"#quick-start","title":"Quick Start","text":"<pre><code>pip install uv\ngit clone https://github.com/jajupmochi/liulian-python.git\ncd liulian-python\nuv pip install -e \".[dev]\"\npython examples/quick_run.py\n</code></pre>"},{"location":"#documentation","title":"Documentation","text":"<ul> <li>Architecture \u2014 Design decisions and module overview</li> <li>Adapter Guide \u2014 How to write model adapters</li> <li>Manifest Specification \u2014 YAML dataset manifest format</li> <li>Contributing \u2014 How to contribute to liulian</li> </ul>"},{"location":"adapter_guide/","title":"Adapter Guide","text":"<p>This guide explains how to write a model adapter for LIULIAN. Adapters are the glue between LIULIAN's unified interface and specific model libraries.</p>"},{"location":"adapter_guide/#what-is-an-adapter","title":"What is an Adapter?","text":"<p>An adapter wraps an external model (e.g., Informer, N-BEATS, GNN) behind the <code>ExecutableModel</code> interface so that LIULIAN's runner can orchestrate it uniformly.</p>"},{"location":"adapter_guide/#adapter-contract-hard-rules","title":"Adapter Contract (Hard Rules)","text":""},{"location":"adapter_guide/#1-single-responsibility","title":"1. Single Responsibility","text":"<p>An adapter only wraps the model. It must NOT contain:</p> <ul> <li>\u274c Training loops</li> <li>\u274c Loss computation</li> <li>\u274c Metric calculation</li> <li>\u274c Data slicing or preprocessing</li> <li>\u274c Logging</li> </ul> <p>It SHOULD contain:</p> <ul> <li>\u2705 Load / initialise the model</li> <li>\u2705 Forward pass</li> <li>\u2705 Save / load checkpoints</li> <li>\u2705 Capability declaration</li> </ul>"},{"location":"adapter_guide/#2-file-size","title":"2. File Size","text":"<p>Recommend \u2264 200 lines of code per adapter file.</p>"},{"location":"adapter_guide/#3-dependency-isolation","title":"3. Dependency Isolation","text":"<p>All 3rd-party imports go through a <code>_vendor.py</code> file:</p> <pre><code># adapters/informer/_vendor.py\ntry:\n    from informer2020 import Informer\nexcept ImportError:\n    raise ImportError(\"Install informer2020: pip install informer2020\")\n</code></pre> <pre><code># adapters/informer/adapter.py\nfrom ._vendor import Informer\nfrom liulian.models.base import ExecutableModel\n</code></pre>"},{"location":"adapter_guide/#4-no-task-specific-logic","title":"4. No Task-Specific Logic","text":"<p>The adapter must be task-agnostic:</p> <pre><code># \u274c FORBIDDEN\ndef forward(self, batch):\n    if self.task.name == 'PredictionTask':\n        return self.predict(batch)\n\n# \u2705 CORRECT\ndef forward(self, batch):\n    return {'predictions': self.model(batch['X'])}\n</code></pre>"},{"location":"adapter_guide/#5-capability-metadata","title":"5. Capability Metadata","text":"<p>Every adapter must declare its capabilities:</p> <pre><code>def capabilities(self) -&gt; Dict[str, bool]:\n    return {\n        'deterministic': True,\n        'probabilistic': False,\n        'uncertainty': False,\n    }\n</code></pre>"},{"location":"adapter_guide/#6-required-test","title":"6. Required Test","text":"<p>Every adapter must have a unit test:</p> <pre><code># tests/adapters/test_my_adapter.py\n\ndef test_forward_shape():\n    model = MyAdapter()\n    model.configure(task, config={})\n    batch = {\"X\": np.random.randn(4, 36, 3).astype(np.float32)}\n    output = model.forward(batch)\n    assert \"predictions\" in output\n    assert output[\"predictions\"].shape == (4, 12, 3)\n</code></pre> <p>Requirements: - Runs on synthetic data only (no real datasets) - No GPU required - Completes in under 1 second</p>"},{"location":"adapter_guide/#example-dummymodel","title":"Example: DummyModel","text":"<p>See <code>liulian/adapters/dummy/adapter.py</code> for a complete reference implementation.</p>"},{"location":"adapter_guide/#file-structure","title":"File Structure","text":"<pre><code>liulian/adapters/\n\u251c\u2500\u2500 __init__.py\n\u251c\u2500\u2500 dummy/\n\u2502   \u251c\u2500\u2500 __init__.py\n\u2502   \u2514\u2500\u2500 adapter.py\n\u2514\u2500\u2500 informer/           # example: future adapter\n    \u251c\u2500\u2500 __init__.py\n    \u251c\u2500\u2500 _vendor.py       # isolate informer import here\n    \u2514\u2500\u2500 adapter.py\n</code></pre>"},{"location":"architecture/","title":"Architecture","text":""},{"location":"architecture/#overview","title":"Overview","text":"<p>LIULIAN follows a task-driven experiment paradigm where the Task is a first-class citizen. Every experiment is defined by the combination of a Task, Dataset, Model (Adapter), and Runner.</p>"},{"location":"architecture/#architecture-diagram","title":"Architecture Diagram","text":"<pre><code>graph TB\n    User[User / Config]\n    Task[Task Layer&lt;br/&gt;PredictionTask]\n    Data[Data Layer&lt;br/&gt;BaseDataset + Manifest]\n    Model[Model Layer&lt;br/&gt;ExecutableModel]\n    Adapter[Adapter Layer&lt;br/&gt;DummyModel, Informer...]\n    Runner[Runtime&lt;br/&gt;Experiment + StateMachine]\n    Optim[Optimizer&lt;br/&gt;RayOptimizer]\n    Logger[Logger&lt;br/&gt;WandB / LocalFile]\n    Artifacts[Artifacts&lt;br/&gt;spec.yaml, metrics, checkpoints]\n\n    User --&gt; Task\n    User --&gt; Data\n    Task --&gt; Runner\n    Data --&gt; Runner\n    Model --&gt; Adapter\n    Adapter --&gt; Runner\n    Runner --&gt; Optim\n    Runner --&gt; Logger\n    Runner --&gt; Artifacts</code></pre>"},{"location":"architecture/#architecture-decisions","title":"Architecture Decisions","text":"Decision Choice Rationale Dependency Management <code>uv</code> Fast, lock-file first, reproducible Configuration Env vars + YAML manifest Stateless for CI/cloud; manifest for data provenance Error Handling Exceptions Simple for MVP; validation errors only Logging stdlib <code>logging</code> + WandB WandB for remote; fallback to local JSON Testing Unit + integration; 60% min Smoke tests for adapters; e2e for runner"},{"location":"architecture/#module-responsibilities","title":"Module Responsibilities","text":""},{"location":"architecture/#task-layer-liuliantasks","title":"Task Layer (<code>liulian/tasks/</code>)","text":"<p>Defines what an experiment is trying to achieve \u2014 loss functions, evaluation metrics, and batch preparation. Tasks are task-agnostic to any specific model.</p>"},{"location":"architecture/#data-layer-liuliandata","title":"Data Layer (<code>liulian/data/</code>)","text":"<p>Manages dataset abstractions, YAML manifests for provenance, data splits, and topology specs for spatiotemporal graphs.</p>"},{"location":"architecture/#model-layer-liulianmodels","title":"Model Layer (<code>liulian/models/</code>)","text":"<p>Provides the <code>ExecutableModel</code> abstract interface that all adapters must implement \u2014 <code>configure()</code>, <code>forward()</code>, <code>save()</code>, <code>load()</code>, <code>capabilities()</code>.</p>"},{"location":"architecture/#adapter-layer-liulianadapters","title":"Adapter Layer (<code>liulian/adapters/</code>)","text":"<p>One adapter per external library. Each adapter wraps a specific model implementation (e.g., DummyModel, Informer) behind the <code>ExecutableModel</code> interface.</p> <p>Contract rules: - Single responsibility (model wrapper only \u2014 no training loop, loss, metrics) - \u2264 200 LOC per adapter - All 3rd-party imports via <code>_vendor.py</code> - Must declare <code>capabilities()</code> - Must have a unit test</p>"},{"location":"architecture/#runtime-layer-liulianruntime","title":"Runtime Layer (<code>liulian/runtime/</code>)","text":"<p>Orchestrates the experiment lifecycle through a state machine (INIT \u2192 TRAIN \u2192 EVAL \u2192 INFER \u2192 COMPLETED) with support for pause/resume and event callbacks.</p>"},{"location":"architecture/#optimizer-layer-liulianoptim","title":"Optimizer Layer (<code>liulian/optim/</code>)","text":"<p>Hyperparameter optimisation via Ray Tune with a fallback grid-sweep when Ray is not installed.</p>"},{"location":"architecture/#logger-layer-liulianloggers","title":"Logger Layer (<code>liulian/loggers/</code>)","text":"<p>Unified logging interface. Full WandB SDK integration with automatic fallback to local JSON logging when WandB is unavailable.</p>"},{"location":"architecture/#plugin-architecture","title":"Plugin Architecture","text":"<p>Domain-specific code (hydrology, traffic, etc.) lives in <code>plugins/</code>, not in the core package. Plugins inherit from core ABCs and are loaded as regular Python imports.</p>"},{"location":"cli/","title":"CLI Usage","text":"<p>liulian provides a command-line interface for running experiments directly from YAML configuration files.</p>"},{"location":"cli/#installation","title":"Installation","text":"<pre><code>pip install -e .\n</code></pre> <p>This registers the <code>liulian</code> console script.</p>"},{"location":"cli/#subcommands","title":"Subcommands","text":""},{"location":"cli/#liulian-info","title":"<code>liulian info</code>","text":"<p>Print version and project tagline.</p> <pre><code>liulian info\n# liulian 0.0.1\n# Liquid Intelligence and Unified Logic for Interactive Adaptive Networks\n# \"Where Space and Time Converge in Intelligence\"\n</code></pre>"},{"location":"cli/#liulian-run-configyaml","title":"<code>liulian run &lt;config.yaml&gt;</code>","text":"<p>Train and evaluate a model from a YAML configuration file.</p> <pre><code>liulian run examples/experiment_dlinear.yaml\n</code></pre> <p>Output:</p> <pre><code>==================================================\n  Status: ok\n  Run ID: example_dlinear_20260211_134348\n  Epochs: 4\n  Best Val MSE: 1.036761\n  Test MSE: 1.042607\n  Test MAE: 0.820369\n  Artifacts: artifacts/example_dlinear_20260211_134348\n==================================================\n</code></pre>"},{"location":"cli/#liulian-eval-configyaml","title":"<code>liulian eval &lt;config.yaml&gt;</code>","text":"<p>Evaluate a pre-trained model without training.</p> <pre><code>liulian eval examples/experiment_dlinear.yaml\n</code></pre>"},{"location":"cli/#options","title":"Options","text":"Flag Description <code>--version</code> Show version number <code>-v</code>, <code>--verbose</code> Enable DEBUG logging"},{"location":"cli/#configuration-file","title":"Configuration File","text":"<p>Experiment configs are YAML files with model and training parameters:</p> <pre><code>name: my_experiment\nmodel: dlinear          # Model adapter name (see Models page)\n\n# Architecture\nseq_len: 96             # Input sequence length\npred_len: 24            # Prediction horizon\nlabel_len: 0            # Label length (0 for most models)\nenc_in: 1               # Encoder input features\ndec_in: 1               # Decoder input features\nc_out: 1                # Output features\nd_model: 64             # Model dimension\nd_ff: 128               # Feed-forward dimension\nn_heads: 4              # Attention heads\ne_layers: 2             # Encoder layers\nd_layers: 1             # Decoder layers\ndropout: 0.1\nembed: timeF            # Embedding type\nfreq: m                 # Frequency string\n\n# Training\ntrain_epochs: 10\nbatch_size: 32\nlearning_rate: 0.001\npatience: 3             # Early stopping patience\n\n# Optional: cap iterations for quick testing\n# max_train_iters: 20\n# max_eval_iters: 10\n\n# Dataset (omit for synthetic data)\n# dataset:\n#   type: SwissRiverDataset\n#   manifest: manifests/swissriver_v1.yaml\n</code></pre>"},{"location":"cli/#available-models","title":"Available Models","text":"<p>Use the <code>model</code> field to select from:</p> Model Name <code>model</code> value Notes DLinear <code>dlinear</code> Fast linear baseline LSTM <code>lstm</code> Classic recurrent PatchTST <code>patchtst</code> Patch-based Transformer iTransformer <code>itransformer</code> Inverted attention Informer <code>informer</code> ProbSparse attention Transformer <code>transformer</code> Vanilla Transformer Autoformer <code>autoformer</code> Auto-correlation FEDformer <code>fedformer</code> Frequency-enhanced TimesNet <code>timesnet</code> 2D temporal variation TimeMixer <code>timemixer</code> Multi-scale mixing TimeXer <code>timexer</code> Cross-variable attention TimeMoE <code>timemoe</code> Mixture of Experts TimeLLM <code>timellm</code> LLM reprogramming Mamba <code>mamba</code> State-space model"},{"location":"cli/#dataset-configuration","title":"Dataset Configuration","text":"<p>Synthetic data (default \u2014 no dataset section): The CLI generates random synthetic data for quick testing.</p> <p>Swiss River dataset: <pre><code>dataset:\n  type: SwissRiverDataset\n  manifest: manifests/swissriver_v1.yaml\n</code></pre></p>"},{"location":"cli/#artifacts","title":"Artifacts","text":"<p>Each run creates an artifacts directory with:</p> <pre><code>artifacts/&lt;name&gt;_&lt;timestamp&gt;/\n\u251c\u2500\u2500 spec.yaml           # Experiment specification\n\u251c\u2500\u2500 metrics.json        # Logged metrics\n\u251c\u2500\u2500 checkpoints/\n\u2502   \u2514\u2500\u2500 checkpoint      # Best model weights\n\u2514\u2500\u2500 ...\n</code></pre>"},{"location":"contributing/","title":"Contributing to LIULIAN","text":"<p>Thank you for your interest in contributing to LIULIAN! This guide will help you get started.</p>"},{"location":"contributing/#development-setup","title":"Development Setup","text":"<pre><code>git clone https://github.com/jajupmochi/liulian-python.git\ncd liulian-python\npip install uv\nuv pip install -e \".[dev,logging]\" --system\n</code></pre>"},{"location":"contributing/#code-style","title":"Code Style","text":"<p>We use the following tools to maintain consistent code quality:</p> <ul> <li>black \u2014 Code formatting (line length 88)</li> <li>isort \u2014 Import sorting (profile: black)</li> <li>flake8 \u2014 Linting</li> <li>mypy \u2014 Static type checking</li> </ul> <p>Run all checks:</p> <pre><code>black --check liulian tests plugins\nisort --check liulian tests plugins\nflake8 liulian tests plugins\nmypy liulian\n</code></pre>"},{"location":"contributing/#running-tests","title":"Running Tests","text":"<pre><code>pytest tests/ -v\npytest --cov=liulian --cov-report=term-missing tests/\n</code></pre> <p>All tests must pass in under 30 seconds. No GPU required.</p>"},{"location":"contributing/#writing-an-adapter","title":"Writing an Adapter","text":"<p>See the Adapter Guide for the full contract. Key rules:</p> <ol> <li>One adapter per external library</li> <li>Inherit from <code>ExecutableModel</code></li> <li>Keep adapter \u2264 200 LOC</li> <li>Use <code>_vendor.py</code> for 3rd-party imports</li> <li>Write a smoke test in <code>tests/adapters/</code></li> </ol>"},{"location":"contributing/#adding-a-domain-plugin","title":"Adding a Domain Plugin","text":"<ol> <li>Create <code>plugins/&lt;domain&gt;/</code> with <code>__init__.py</code> and adapter module</li> <li>Inherit from <code>BaseDataset</code> or <code>ExecutableModel</code></li> <li>Include a manifest YAML if applicable</li> <li>Add tests that run without the plugin's external dependencies</li> </ol>"},{"location":"contributing/#pull-request-process","title":"Pull Request Process","text":"<ol> <li>Fork the repository</li> <li>Create a feature branch: <code>git checkout -b feature/my-feature</code></li> <li>Write tests for any new functionality</li> <li>Ensure all tests pass and coverage \u2265 60%</li> <li>Run code formatters: <code>black . &amp;&amp; isort .</code></li> <li>Submit a pull request with a clear description</li> </ol>"},{"location":"contributing/#keeping-readmezhmd-in-sync","title":"Keeping README.zh.md in Sync","text":"<p>When updating <code>README.md</code>, please also update <code>README.zh.md</code> to keep the Chinese translation current. If you are not comfortable translating, note the change in your PR and a maintainer will handle it.</p>"},{"location":"contributing/#commit-message-convention","title":"Commit Message Convention","text":"<p>Use conventional commits:</p> <pre><code>feat: add new adapter for ModelX\nfix: correct batch slicing in PredictionTask\ndocs: update architecture diagram\ntest: add smoke test for InformerAdapter\n</code></pre>"},{"location":"contributing/#questions","title":"Questions?","text":"<p>Open a GitHub issue or start a discussion. We welcome all contributions, from bug reports to new adapters.</p>"},{"location":"data_loaders/","title":"PyTorch Data Loaders","text":"<p>\u7eafPyTorch\u5f20\u91cf\u7684\u65f6\u95f4\u5e8f\u5217\u6570\u636e\u52a0\u8f7d\u5668,\u4eceTime-Series-Library\u9002\u914d\u800c\u6765\u3002</p>"},{"location":"data_loaders/#_1","title":"\u7279\u6027","text":"<ul> <li>\u2705 \u7eafPyTorch\u5f20\u91cf: \u6240\u6709\u6570\u636e\u52a0\u8f7d\u5668\u76f4\u63a5\u8fd4\u56de<code>torch.Tensor</code>,\u65e0\u9700numpy\u8f6c\u6362</li> <li>\u2705 \u6807\u51c6\u5f52\u4e00\u5316: \u4f7f\u7528<code>StandardScaler</code>\u8fdb\u884c\u6570\u636e\u5f52\u4e00\u5316</li> <li>\u2705 \u591a\u79cd\u5206\u5272: \u652f\u6301train/val/test\u81ea\u52a8\u5206\u5272</li> <li>\u2705 \u65f6\u95f4\u7f16\u7801: \u4e24\u79cd\u6a21\u5f0f - categorical (\u6708/\u65e5/\u65f6) \u548c time_features</li> <li>\u2705 \u591a\u79cd\u7279\u5f81\u6a21\u5f0f: M (\u591a\u53d8\u91cf), S (\u5355\u53d8\u91cf), MS (\u591a\u53d8\u91cf\u5230\u5355\u53d8\u91cf)</li> <li>\u2705 \u5de5\u5382\u6a21\u5f0f: \u901a\u8fc7\u540d\u79f0\u52a8\u6001\u521b\u5efa\u6570\u636e\u52a0\u8f7d\u5668</li> <li>\u2705 \u81ea\u5b9a\u4e49\u6570\u636e\u96c6: \u652f\u6301\u4efb\u610fCSV\u683c\u5f0f</li> </ul>"},{"location":"data_loaders/#_2","title":"\u6570\u636e\u96c6\u7c7b","text":""},{"location":"data_loaders/#etthourdataset","title":"ETTHourDataset","text":"<p>Electricity Transformer Temperature (ETT) \u5c0f\u65f6\u7ea7\u6570\u636e\u96c6\u3002</p> <pre><code>from liulian.data.torch_datasets import ETTHourDataset\n\ndataset = ETTHourDataset(\n    root_path='./data/ETT',\n    data_path='ETTh1.csv',\n    flag='train',  # 'train', 'val', 'test'\n    size=(96, 48, 96),  # (seq_len, label_len, pred_len)\n    features='M',  # 'M', 'S', 'MS'\n    scale=True,\n)\n\n# Get a sample - all torch tensors!\nseq_x, seq_y, seq_x_mark, seq_y_mark = dataset[0]\n</code></pre> <p>\u5206\u5272\u914d\u7f6e: - Train: \u524d12\u4e2a\u6708 - Val: \u63a5\u4e0b\u67654\u4e2a\u6708 - Test: \u6700\u540e4\u4e2a\u6708</p>"},{"location":"data_loaders/#ettminutedataset","title":"ETTMinuteDataset","text":"<p>ETT 15\u5206\u949f\u7ea7\u6570\u636e\u96c6 (\u4e0eETTHourDataset\u7c7b\u4f3c,\u4f46\u652f\u6301\u5206\u949f\u7ea7\u65f6\u95f4\u7279\u5f81)\u3002</p> <pre><code>from liulian.data.torch_datasets import ETTMinuteDataset\n\ndataset = ETTMinuteDataset(\n    root_path='./data/ETT',\n    data_path='ETTm1.csv',\n    flag='train',\n    freq='t',  # 15-minute frequency\n)\n</code></pre>"},{"location":"data_loaders/#customcsvdataset","title":"CustomCSVDataset","text":"<p>\u901a\u7528CSV\u6570\u636e\u96c6,\u652f\u6301\u4efb\u610fCSV\u683c\u5f0f\u3002</p> <pre><code>from liulian.data.torch_datasets import CustomCSVDataset\n\ndataset = CustomCSVDataset(\n    root_path='./data',\n    data_path='my_data.csv',\n    flag='train',\n    target='target_column',  # Your target column name\n    train_ratio=0.7,  # Customizable split\n    test_ratio=0.2,\n)\n</code></pre> <p>CSV\u8981\u6c42: - \u5fc5\u987b\u5305\u542b <code>date</code> \u5217 - \u5fc5\u987b\u5305\u542b\u76ee\u6807\u5217 (\u901a\u8fc7<code>target</code>\u53c2\u6570\u6307\u5b9a) - \u5176\u4ed6\u5217\u81ea\u52a8\u4f5c\u4e3a\u7279\u5f81</p>"},{"location":"data_loaders/#_3","title":"\u5de5\u5382\u51fd\u6570","text":""},{"location":"data_loaders/#create_dataloader","title":"create_dataloader","text":"<p>\u521b\u5efa\u5355\u4e2aDataLoader:</p> <pre><code>from liulian.data.data_factory import create_dataloader\n\nloader = create_dataloader(\n    data_name='ETTh1',  # 'ETTh1', 'ETTh2', 'ETTm1', 'ETTm2', 'custom'\n    root_path='./data/ETT',\n    data_path='ETTh1.csv',\n    flag='train',\n    size=(96, 48, 96),\n    batch_size=32,\n    shuffle=True,\n)\n\n# Iterate through batches\nfor batch in loader:\n    seq_x, seq_y, seq_x_mark, seq_y_mark = batch\n    # All are torch.Tensor!\n</code></pre>"},{"location":"data_loaders/#create_dataloaders","title":"create_dataloaders","text":"<p>\u4e00\u6b21\u521b\u5efa\u6240\u6709\u5206\u5272:</p> <pre><code>from liulian.data.data_factory import create_dataloaders\n\nloaders = create_dataloaders(\n    data_name='ETTh1',\n    root_path='./data/ETT',\n    data_path='ETTh1.csv',\n    size=(96, 48, 96),\n    batch_size=32,\n)\n\ntrain_loader = loaders['train']\nval_loader = loaders['val']\ntest_loader = loaders['test']\n</code></pre>"},{"location":"data_loaders/#_4","title":"\u53c2\u6570\u8bf4\u660e","text":"\u53c2\u6570 \u7c7b\u578b \u9ed8\u8ba4\u503c \u8bf4\u660e <code>root_path</code> str - \u6570\u636e\u6587\u4ef6\u6839\u76ee\u5f55 <code>data_path</code> str - CSV\u6587\u4ef6\u540d <code>flag</code> str 'train' \u5206\u5272\u7c7b\u578b: 'train', 'val', 'test' <code>size</code> tuple (96, 48, 96) (seq_len, label_len, pred_len) <code>features</code> str 'M' \u7279\u5f81\u6a21\u5f0f: 'M', 'S', 'MS' <code>target</code> str 'OT' \u76ee\u6807\u5217\u540d\u79f0 <code>scale</code> bool True \u662f\u5426\u4f7f\u7528StandardScaler\u5f52\u4e00\u5316 <code>timeenc</code> int 0 \u65f6\u95f4\u7f16\u7801: 0 (categorical), 1 (time_features) <code>freq</code> str 'h' \u9891\u7387\u5b57\u7b26\u4e32: 'h' (\u5c0f\u65f6), 't' (15\u5206\u949f) <code>batch_size</code> int 32 DataLoader\u6279\u5927\u5c0f <code>shuffle</code> bool True \u662f\u5426\u6253\u4e71\u6570\u636e"},{"location":"data_loaders/#_5","title":"\u8fd4\u56de\u683c\u5f0f","text":"<p>\u6240\u6709\u6570\u636e\u52a0\u8f7d\u5668\u8fd4\u56de4\u4e2atorch.Tensor:</p> <pre><code>seq_x, seq_y, seq_x_mark, seq_y_mark = batch\n\n# seq_x: \u8f93\u5165\u5e8f\u5217 [batch_size, seq_len, features]\n# seq_y: \u76ee\u6807\u5e8f\u5217 [batch_size, label_len + pred_len, features]  \n# seq_x_mark: \u8f93\u5165\u65f6\u95f4\u7279\u5f81 [batch_size, seq_len, time_dim]\n# seq_y_mark: \u76ee\u6807\u65f6\u95f4\u7279\u5f81 [batch_size, label_len + pred_len, time_dim]\n</code></pre>"},{"location":"data_loaders/#_6","title":"\u4e0e\u6a21\u578b\u96c6\u6210","text":"<p>\u6570\u636e\u52a0\u8f7d\u5668\u8fd4\u56de\u7684\u5f20\u91cf\u53ef\u4ee5\u76f4\u63a5\u4f20\u9012\u7ed9PyTorch\u6a21\u578b\u9002\u914d\u5668:</p> <pre><code>from liulian.data.data_factory import create_dataloader\nfrom liulian.models.torch.dlinear_adapter import DLinearAdapter\n\n# Load data\nloader = create_dataloader(\n    data_name='ETTh1',\n    root_path='./data/ETT',\n    data_path='ETTh1.csv',\n    flag='test',\n    batch_size=32,\n)\n\n# Create model adapter\nmodel_adapter = DLinearAdapter(model, config)\n\n# Run inference - NO numpy conversion!\nfor batch in loader:\n    seq_x, seq_y, seq_x_mark, seq_y_mark = batch\n\n    # Prepare model input\n    model_input = {\n        'x_enc': seq_x,\n        'x_mark_enc': seq_x_mark,\n        'x_dec': seq_y[:, :config['label_len'], :],\n        'x_mark_dec': seq_y_mark,\n    }\n\n    # Forward pass - pure torch tensors throughout!\n    output = model_adapter.forward(model_input)\n    predictions = output['predictions']\n</code></pre>"},{"location":"data_loaders/#_7","title":"\u6d4b\u8bd5","text":"<p>\u8fd0\u884c\u5b8c\u6574\u6d4b\u8bd5\u5957\u4ef6:</p> <pre><code>pytest tests/data/test_torch_datasets.py -v\n</code></pre> <p>\u6d4b\u8bd5\u8986\u76d6: - \u2705 \u57fa\u672c\u52a0\u8f7d\u529f\u80fd - \u2705 \u8fd4\u56detorch.Tensor\u7c7b\u578b - \u2705 \u5f20\u91cf\u5f62\u72b6\u9a8c\u8bc1 - \u2705 \u5355\u53d8\u91cf/\u591a\u53d8\u91cf\u6a21\u5f0f - \u2705 inverse_transform - \u2705 train/val/test\u5206\u5272 - \u2705 \u5de5\u5382\u6a21\u5f0f - \u2705 \u7aef\u5230\u7aefpipeline</p> <p>\u6d4b\u8bd5\u7ed3\u679c: 18/18 \u901a\u8fc7 \u2705</p>"},{"location":"data_loaders/#_8","title":"\u6587\u4ef6\u7ed3\u6784","text":"<pre><code>liulian/data/\n\u251c\u2500\u2500 torch_datasets.py       # Dataset classes (~560 lines)\n\u2502   \u251c\u2500\u2500 ETTHourDataset\n\u2502   \u251c\u2500\u2500 ETTMinuteDataset\n\u2502   \u2514\u2500\u2500 CustomCSVDataset\n\u2514\u2500\u2500 data_factory.py         # Factory functions (~220 lines)\n    \u251c\u2500\u2500 create_dataloader()\n    \u251c\u2500\u2500 create_dataloaders()\n    \u2514\u2500\u2500 register_dataset()\n\ntests/data/\n\u2514\u2500\u2500 test_torch_datasets.py  # Test suite (~550 lines, 18 tests)\n\nexamples/\n\u2514\u2500\u2500 data_loaders_example.py # Usage examples\n</code></pre>"},{"location":"data_loaders/#_9","title":"\u6ce8\u518c\u81ea\u5b9a\u4e49\u6570\u636e\u96c6","text":"<p>\u53ef\u4ee5\u6ce8\u518c\u81ea\u5df1\u7684\u6570\u636e\u96c6\u7c7b:</p> <pre><code>from liulian.data.data_factory import register_dataset\nfrom torch.utils.data import Dataset\n\nclass MyCustomDataset(Dataset):\n    def __init__(self, root_path, data_path, flag, **kwargs):\n        # Your implementation\n        pass\n\n    def __getitem__(self, index):\n        # Return (seq_x, seq_y, seq_x_mark, seq_y_mark)\n        pass\n\n    def __len__(self):\n        return self.num_samples\n\n# Register\nregister_dataset('my_dataset', MyCustomDataset)\n\n# Use\nloader = create_dataloader('my_dataset', ...)\n</code></pre>"},{"location":"data_loaders/#_10","title":"\u4f9d\u8d56\u9879","text":"<ul> <li>Python 3.12+</li> <li>PyTorch 2.10+</li> <li>pandas</li> <li>numpy</li> <li>scikit-learn</li> </ul>"},{"location":"data_loaders/#_11","title":"\u8bb8\u53ef","text":"<p>MIT License (\u9002\u914d\u81eaTime-Series-Library)</p>"},{"location":"data_loaders/#_12","title":"\u67b6\u6784\u51b3\u7b56","text":"<p>\u4e3a\u4ec0\u4e48\u4e0d\u8f6c\u6362numpy? \u4f20\u7edf\u5b9e\u73b0\u5728\u6570\u636e\u52a0\u8f7d\u540e\u8f6c\u6362\u4e3anumpy,\u7136\u540e\u5728\u6a21\u578b\u8f93\u5165\u65f6\u518d\u8f6c\u6362\u56detorch\u3002\u8fd9\u4f1a: - \u589e\u52a0\u4e0d\u5fc5\u8981\u7684\u5f00\u9500 - \u7834\u574f\u68af\u5ea6\u6d41 - \u4f7f\u4ee3\u7801\u66f4\u590d\u6742</p> <p>\u6211\u4eec\u7684\u5b9e\u73b0\u76f4\u63a5\u8fd4\u56detorch.Tensor,\u6574\u4e2apipeline\u4fdd\u6301\u7eafPyTorch: <pre><code>CSV \u2192 pandas \u2192 torch.Tensor \u2192 Model \u2192 torch.Tensor\n</code></pre></p> <p>COPY-PASTE\u539f\u5219: \u6211\u4eec\u4ece\u53c2\u8003\u9879\u76ee\u590d\u5236\u4e86\u6838\u5fc3\u903b\u8f91,\u4f46\u8fdb\u884c\u4e86torch\u9002\u914d,\u800c\u4e0d\u662f\u521b\u5efa\u62bd\u8c61\u5c42\u3002\u8fd9\u786e\u4fdd\u4e86: - \u4e0e\u53c2\u8003\u5b9e\u73b0\u7684\u4e00\u81f4\u6027 - \u66f4\u5bb9\u6613\u8c03\u8bd5\u548c\u7ef4\u62a4 - \u6e05\u6670\u7684\u4ee3\u7801\u6eaf\u6e90</p>"},{"location":"manifest_spec/","title":"Manifest Specification","text":"<p>A manifest is a YAML file that describes a dataset's provenance, structure, and preprocessing steps. It is used by LIULIAN for data validation, reproducibility, and provenance tracking.</p>"},{"location":"manifest_spec/#required-fields","title":"Required Fields","text":"Field Type Description <code>name</code> string Unique dataset identifier <code>version</code> string Semantic version of the dataset <code>fields</code> list List of field descriptors (see below) <code>splits</code> dict Mapping of split names to date ranges or indices"},{"location":"manifest_spec/#optional-fields","title":"Optional Fields","text":"Field Type Description <code>source</code> string URL or DOI of the original data source <code>hash</code> string SHA-256 hash for integrity verification <code>description</code> string Human-readable dataset description <code>preprocessing</code> list Ordered list of preprocessing steps applied <code>topology</code> dict Graph / spatial topology metadata"},{"location":"manifest_spec/#field-descriptor","title":"Field Descriptor","text":"<p>Each entry in <code>fields</code> must have:</p> Key Type Required Description <code>name</code> string \u2705 Column / variable identifier <code>dtype</code> string \u2705 Numpy dtype (e.g. <code>float32</code>, <code>int64</code>) <code>unit</code> string \u274c Physical unit (e.g. <code>m3/s</code>, <code>degC</code>) <code>semantic_tags</code> list \u274c Free-form tags (e.g. <code>[target]</code>, <code>[feature]</code>)"},{"location":"manifest_spec/#topology-section","title":"Topology Section","text":"<p>For spatiotemporal datasets with graph structure:</p> <pre><code>topology:\n  node_ids: [\"S1\", \"S2\", \"S3\"]\n  edges:\n    - [\"S1\", \"S2\"]\n    - [\"S2\", \"S3\"]\n  coordinates:\n    S1: [46.95, 7.45]\n    S2: [46.80, 7.50]\n    S3: [46.65, 7.55]\n</code></pre>"},{"location":"manifest_spec/#full-example","title":"Full Example","text":"<pre><code>name: swissriver-v1\nsource: https://doi.org/10.5281/zenodo.example\nversion: \"1.0\"\nhash: \"sha256:abc123...\"\ndescription: &gt;\n  Swiss river network discharge dataset with hourly measurements.\n\npreprocessing:\n  - name: resample\n    params:\n      freq: \"1H\"\n  - name: fillna\n    params:\n      method: linear\n\nsplits:\n  train:\n    start: \"2010-01-01\"\n    end: \"2018-12-31\"\n  val:\n    start: \"2019-01-01\"\n    end: \"2019-12-31\"\n  test:\n    start: \"2020-01-01\"\n    end: \"2020-12-31\"\n\ntopology:\n  node_ids: [\"S1\", \"S2\", \"S3\", \"S4\", \"S5\"]\n  edges:\n    - [\"S1\", \"S2\"]\n    - [\"S2\", \"S3\"]\n    - [\"S3\", \"S4\"]\n    - [\"S4\", \"S5\"]\n\nfields:\n  - name: discharge\n    dtype: float32\n    unit: m3/s\n    semantic_tags: [target]\n  - name: precipitation\n    dtype: float32\n    unit: mm/h\n    semantic_tags: [feature]\n</code></pre>"},{"location":"manifest_spec/#validation","title":"Validation","text":"<p>Use <code>validate_manifest()</code> to check a manifest dict for errors:</p> <pre><code>from liulian.data.manifest import load_manifest, validate_manifest\n\nmanifest = load_manifest(\"manifests/swissriver_v1.yaml\")\nerrors = validate_manifest(manifest)\nif errors:\n    print(\"Manifest errors:\", errors)\n</code></pre>"},{"location":"models/torch_models/","title":"PyTorch Time Series Models","text":"<p>This guide documents the 14 time series forecasting models adapted from Time-Series-Library and Time-LLM into the liulian framework.</p>"},{"location":"models/torch_models/#overview","title":"Overview","text":"<p>All adapted models follow a consistent adapter pattern that bridges PyTorch implementations to liulian's <code>ExecutableModel</code> interface. This enables seamless integration while preserving the original model algorithms.</p>"},{"location":"models/torch_models/#adapted-models","title":"Adapted Models","text":"Model Paper Year Key Innovation Complexity Use Case LSTM Classic \u2014 Recurrent baseline O(L) Simple baseline DLinear AAAI'23 2023 Simple linear layers O(L) Fast baseline Informer AAAI'21 2021 ProbSparse attention O(L log L) Long sequences Autoformer NeurIPS'21 2021 AutoCorrelation O(L log L) Seasonal data Transformer Classic 2017 Self-attention O(L\u00b2) General purpose FEDformer ICML'22 2022 Frequency enhanced O(L) Seasonal data PatchTST ICLR'23 2023 Patch-based tokens O(N\u00b2), N=patches State-of-the-art iTransformer ICLR'24 2024 Inverted attention O(V\u00b2L), V=variates Multivariate TimesNet ICLR'23 2023 2D temporal variation O(L\u00b7K) Multi-task SOTA TimeMixer ICLR'24 2024 Multi-scale mixing O(L) Efficient mixing TimeXer NeurIPS'24 2024 Exogenous variables O(L\u00b2) Cross-variable Mamba 2024 2024 Selective SSM O(L) Long sequences TimeLLM ICLR'24 2024 LLM reprogramming O(L) + LLM Novel approach TimeMoE 2024 2024 Mixture of Experts O(L log L) Zero-shot"},{"location":"models/torch_models/#swiss-river-benchmark-results","title":"Swiss River Benchmark Results","text":"<p>8 models tested on the Swiss River hydrological dataset (seq_len=30, pred_len=7, quick test: 2 epochs, 20 iters):</p> Model Params MSE MAE RMSE Time (s) PatchTST 73K 0.0060 0.0633 0.0777 0.9 TimeXer 110K 0.0078 0.0738 0.0881 0.9 iTransformer 73K 0.0082 0.0738 0.0905 0.8 DLinear 1.3K 0.0082 0.0760 0.0908 0.4 TimesNet 9.4M 0.0233 0.1305 0.1527 107.3 Transformer 118K 0.0558 0.2146 0.2362 1.8 Informer 130K 0.0584 0.2056 0.2416 1.7 LSTM 51K 0.0905 0.2728 0.3008 13.4 <p>Incompatible Models</p> <p>Autoformer, FEDformer, and TimeMixer require <code>label_len &gt; 0</code> for decoder mark construction, which is incompatible with the Swiss River DataLoader's current format. These models work with other datasets (e.g., ETT).</p>"},{"location":"models/torch_models/#installation","title":"Installation","text":""},{"location":"models/torch_models/#basic-installation-all-models-except-timellmtimemoe","title":"Basic Installation (All Models Except TimeLLM/TimeMoE)","text":"<pre><code>pip install -e \".[torch-models]\"\n</code></pre> <p>This installs: - <code>torch &gt;= 2.0.0</code> - All layer dependencies</p>"},{"location":"models/torch_models/#full-installation-including-timellmtimemoe","title":"Full Installation (Including TimeLLM/TimeMoE)","text":"<pre><code>pip install -e \".[torch-models-full]\"\n</code></pre> <p>Additional dependencies: - <code>transformers &gt;= 4.0.0</code> (HuggingFace models) - <code>accelerate &gt;= 0.20.0</code> (efficient model loading) - <code>einops &gt;= 0.6.0</code> (tensor operations)</p> <p>\u26a0\ufe0f Warning: TimeLLM downloads multi-GB pretrained language models on first use.</p>"},{"location":"models/torch_models/#quick-start","title":"Quick Start","text":""},{"location":"models/torch_models/#basic-forecasting-example","title":"Basic Forecasting Example","text":"<pre><code>from liulian.models.torch.dlinear import DLinearAdapter\nimport numpy as np\n\n# Configure model\nconfig = {\n    \"task_name\": \"forecast\",\n    \"seq_len\": 96,          # Input sequence length\n    \"pred_len\": 24,         # Forecast horizon\n    \"label_len\": 48,        # Label length\n    \"enc_in\": 7,            # Number of input features\n    \"dec_in\": 7,            # Decoder input features\n    \"c_out\": 7,             # Output channels\n    \"individual\": False,    # Shared linear layers\n}\n\n# Create model\nmodel = DLinearAdapter(config)\n\n# Prepare input data (NumPy arrays)\ninputs = {\n    \"x_enc\": np.random.randn(32, 96, 7),      # (batch, seq_len, features)\n    \"x_mark_enc\": np.random.randn(32, 96, 4), # Time features\n    \"x_dec\": np.random.randn(32, 72, 7),      # Decoder input (label_len + pred_len)\n    \"x_mark_dec\": np.random.randn(32, 72, 4), # Decoder time features\n}\n\n# Run prediction\noutputs = model.run(inputs)\npredictions = outputs[\"predictions\"]  # Shape: (32, 24, 7)\n</code></pre>"},{"location":"models/torch_models/#model-specific-guides","title":"Model-Specific Guides","text":""},{"location":"models/torch_models/#1-dlinear-simple-linear-baseline","title":"1. DLinear - Simple Linear Baseline","text":"<p>Best for: Fast prototyping, baselines, resource-constrained environments</p> <p>Key Features: - Extremely simple: decomposition + linear layers - Very fast training and inference - Surprisingly effective despite simplicity - Supports all 4 tasks (forecast, imputation, anomaly, classification)</p> <p>Configuration:</p> <pre><code>config = {\n    \"task_name\": \"forecast\",    # or \"imputation\", \"anomaly_detection\", \"classification\"\n    \"seq_len\": 96,\n    \"pred_len\": 24,\n    \"label_len\": 48,\n    \"enc_in\": 7,                # Number of input features\n    \"dec_in\": 7,\n    \"c_out\": 7,\n    \"individual\": False,        # False: shared layers, True: per-channel layers\n}\n</code></pre> <p>Usage Tips: - Start with <code>individual=False</code> for parameter efficiency - Try <code>individual=True</code> if channels have very different dynamics - Good baseline before trying complex models</p> <p>Example:</p> <pre><code>from liulian.models.torch.dlinear import DLinearAdapter\n\nmodel = DLinearAdapter(config)\noutputs = model.run(inputs)\n</code></pre>"},{"location":"models/torch_models/#2-informer-efficient-long-sequence-forecasting","title":"2. Informer - Efficient Long Sequence Forecasting","text":"<p>Best for: Long sequences (&gt;500 time steps), memory-constrained scenarios</p> <p>Key Features: - ProbSparse attention: O(L log L) complexity - Distillation: reduces memory footprint - Encoder-decoder architecture - Efficient for very long sequences</p> <p>Configuration:</p> <pre><code>config = {\n    \"task_name\": \"forecast\",\n    \"seq_len\": 512,             # Can handle long sequences\n    \"pred_len\": 96,\n    \"label_len\": 48,\n    \"enc_in\": 7,\n    \"dec_in\": 7,\n    \"c_out\": 7,\n    \"d_model\": 512,             # Model dimension\n    \"n_heads\": 8,               # Attention heads\n    \"e_layers\": 2,              # Encoder layers\n    \"d_layers\": 1,              # Decoder layers\n    \"d_ff\": 2048,               # Feed-forward dimension\n    \"factor\": 5,                # ProbSparse sampling factor (higher = more sparse)\n    \"distil\": True,             # Use distillation (recommended)\n    \"dropout\": 0.1,\n    \"activation\": \"gelu\",\n    \"embed\": \"timeF\",           # Time feature embedding\n    \"freq\": \"h\",                # 'h'=hourly, 'd'=daily, etc.\n}\n</code></pre> <p>Usage Tips: - Use <code>factor=5</code> as default, increase for longer sequences - Keep <code>distil=True</code> for memory efficiency - Encoder-only version: set <code>d_layers=0</code></p> <p>Example:</p> <pre><code>from liulian.models.torch.informer import InformerAdapter\n\nmodel = InformerAdapter(config)\noutputs = model.run(inputs)\n</code></pre>"},{"location":"models/torch_models/#3-autoformer-decomposition-with-autocorrelation","title":"3. Autoformer - Decomposition with AutoCorrelation","text":"<p>Best for: Data with clear seasonal patterns, energy/weather forecasting</p> <p>Key Features: - AutoCorrelation: discovers period-based dependencies via FFT - Progressive decomposition: separates trend and seasonal at each layer - O(L log L) complexity - Excellent for periodic data</p> <p>Configuration:</p> <pre><code>config = {\n    \"task_name\": \"forecast\",\n    \"seq_len\": 96,\n    \"pred_len\": 24,\n    \"label_len\": 48,\n    \"enc_in\": 7,\n    \"dec_in\": 7,\n    \"c_out\": 7,\n    \"d_model\": 512,\n    \"n_heads\": 8,\n    \"e_layers\": 2,\n    \"d_layers\": 1,\n    \"d_ff\": 2048,\n    \"moving_avg\": 25,           # Decomposition kernel size (match periodicity)\n    \"factor\": 1,                # AutoCorrelation factor\n    \"dropout\": 0.1,\n    \"activation\": \"gelu\",\n    \"embed\": \"timeF\",\n    \"freq\": \"h\",\n}\n</code></pre> <p>Usage Tips: - Set <code>moving_avg</code> to match your data's seasonality:   - Hourly data: 24-25 (daily period)   - Daily data: 7 (weekly) or 30 (monthly)   - Monthly data: 12 (yearly) - Best for data with clear repeating patterns - Visualize decomposition to verify trend/seasonal separation</p> <p>Example:</p> <pre><code>from liulian.models.torch.autoformer import AutoformerAdapter\n\n# Hourly electricity data with daily seasonality\nconfig[\"moving_avg\"] = 24\nmodel = AutoformerAdapter(config)\noutputs = model.run(inputs)\n</code></pre>"},{"location":"models/torch_models/#4-patchtst-patch-based-transformer","title":"4. PatchTST - Patch-Based Transformer","text":"<p>Best for: State-of-the-art accuracy, benchmark comparisons</p> <p>Key Features: - Treats patches (not points) as tokens - Channel-independent processing - Reversible instance normalization (RevIN) - Often achieves best results on benchmarks</p> <p>Configuration:</p> <pre><code>config = {\n    \"task_name\": \"forecast\",\n    \"seq_len\": 512,             # Should be divisible by stride\n    \"pred_len\": 96,\n    \"label_len\": 48,\n    \"enc_in\": 7,\n    \"dec_in\": 7,\n    \"c_out\": 7,\n    \"d_model\": 128,\n    \"n_heads\": 16,\n    \"e_layers\": 3,\n    \"d_ff\": 256,\n    \"dropout\": 0.1,\n    \"activation\": \"gelu\",\n    \"patch_len\": 16,            # Length of each patch\n    \"stride\": 8,                # Patch extraction stride\n    \"padding_patch\": \"end\",     # Padding strategy\n    \"revin\": True,              # Use reversible normalization (HIGHLY RECOMMENDED)\n    \"affine\": False,\n    \"subtract_last\": False,\n    \"individual\": False,        # False: shared, True: channel-independent heads\n}\n</code></pre> <p>Usage Tips: - Always use <code>revin=True</code> (major performance boost) - Patch length selection:   - 8-16: Fine-grained patterns   - 24-32: Smoother, faster   - Rule of thumb: <code>patch_len ~ seasonality / 4</code> - Stride selection:   - <code>stride = patch_len</code>: Non-overlapping (faster)   - <code>stride = patch_len // 2</code>: Overlapping (better) - <code>individual=True</code> for heterogeneous channels, <code>False</code> for efficiency</p> <p>Example:</p> <pre><code>from liulian.models.torch.patchtst import PatchTSTAdapter\n\n# Typical configuration for daily forecasting\nconfig = {\n    \"seq_len\": 512,\n    \"pred_len\": 96,\n    \"patch_len\": 16,\n    \"stride\": 8,\n    \"revin\": True,  # Critical!\n    # ... other params\n}\n\nmodel = PatchTSTAdapter(config)\noutputs = model.run(inputs)\n</code></pre>"},{"location":"models/torch_models/#5-itransformer-inverted-transformer","title":"5. iTransformer - Inverted Transformer","text":"<p>Best for: Multivariate data with strong inter-variate correlations</p> <p>Key Features: - Attention across variates (not time) - Inverted architecture - Non-stationary normalization - Captures variate dependencies effectively</p> <p>Configuration:</p> <pre><code>config = {\n    \"task_name\": \"forecast\",\n    \"seq_len\": 96,\n    \"pred_len\": 24,\n    \"label_len\": 48,\n    \"enc_in\": 21,               # Works well with many variates\n    \"dec_in\": 21,\n    \"c_out\": 21,\n    \"d_model\": 512,\n    \"n_heads\": 8,\n    \"e_layers\": 2,\n    \"d_ff\": 2048,\n    \"dropout\": 0.1,\n    \"activation\": \"gelu\",\n    \"use_norm\": True,           # Non-stationary normalization\n}\n</code></pre> <p>Usage Tips: - Best for datasets with many correlated variables (V &gt; 10) - Complexity O(V\u00b2L) - scales quadratically with number of variates - Keep <code>use_norm=True</code> for non-stationary data - May be overkill for univariate forecasting</p> <p>Example:</p> <pre><code>from liulian.models.torch.itransformer import iTransformerAdapter\n\n# Multivariate weather forecasting (21 variables)\nmodel = iTransformerAdapter(config)\noutputs = model.run(inputs)\n</code></pre>"},{"location":"models/torch_models/#6-timellm-llm-based-forecasting","title":"6. TimeLLM - LLM-Based Forecasting","text":"<p>Best for: Research, exploring LLM capabilities, domain-specific forecasting</p> <p>Key Features: - Reprograms frozen language models (LLAMA/GPT2/BERT) - Converts time series to text-like embeddings - Leverages pre-trained knowledge - Novel cross-modal approach</p> <p>\u26a0\ufe0f Important Warnings: - Large downloads: 500MB (GPT2/BERT) to 7GB (LLAMA) - GPU required: 8-16GB GPU memory - Forecasting only: No other tasks supported - Slow inference: LLM forward pass adds overhead</p> <p>Configuration:</p> <pre><code>config = {\n    # LLM Configuration\n    \"llm_model\": \"GPT2\",        # 'LLAMA', 'GPT2', or 'BERT'\n    \"llm_dim\": 768,             # LLAMA:4096, GPT2:768, BERT:768\n    \"llm_layers\": 6,            # Number of LLM layers to use\n\n    # Time Series Configuration\n    \"seq_len\": 512,\n    \"pred_len\": 96,\n    \"enc_in\": 7,\n    \"d_model\": 32,              # Patch embedding dimension\n    \"d_ff\": 128,\n    \"patch_len\": 16,\n    \"stride\": 8,\n    \"c_out\": 7,\n    \"dropout\": 0.1,\n\n    # Optional Domain Knowledge\n    \"prompt_domain\": True,\n    \"content\": \"electricity load\",  # Domain description\n}\n</code></pre> <p>LLM Selection Guide:</p> LLM Download Size GPU Memory Performance Speed BERT ~500MB 4-8GB \u2b50\u2b50\u2b50 Fast GPT2 ~500MB 4-8GB \u2b50\u2b50\u2b50\u2b50 Medium LLAMA ~7GB 8-16GB \u2b50\u2b50\u2b50\u2b50\u2b50 Slow <p>Usage Tips: - Start with GPT2 for testing (good balance) - Use LLAMA only if you have resources and need max accuracy - First run takes 5-30 minutes (model download) - Subsequent runs use cached models (<code>~/.cache/huggingface/</code>) - Enable <code>prompt_domain</code> with descriptive text for better results</p> <p>Example:</p> <pre><code>from liulian.models.torch.timellm import TimeLLMAdapter\n\n# Start with GPT2 for testing\nconfig = {\n    \"llm_model\": \"GPT2\",\n    \"llm_dim\": 768,\n    \"llm_layers\": 6,\n    \"seq_len\": 512,\n    \"pred_len\": 96,\n    \"patch_len\": 16,\n    \"stride\": 8,\n    \"prompt_domain\": True,\n    \"content\": \"weather\",\n    # ... other params\n}\n\nmodel = TimeLLMAdapter(config)\n# First run: Downloads GPT2 (~500MB)\n# Subsequent runs: Loads from cache\noutputs = model.run(inputs)\n</code></pre>"},{"location":"models/torch_models/#7-timemoe-zero-shot-mixture-of-experts","title":"7. TimeMoE - Zero-Shot Mixture of Experts","text":"<p>Best for: Zero-shot forecasting, quick deployment without training</p> <p>Key Features: - Pretrained Mixture of Experts model - Zero-shot: no training needed - Uses Maple728/TimeMoE-50M from HuggingFace - Forecasting only</p> <p>\u26a0\ufe0f Important Warnings: - Download: ~200MB pretrained model on first use - Forecasting only: No other tasks supported</p> <p>Configuration:</p> <pre><code>config = {\n    \"model_name\": \"Maple728/TimeMoE-50M\",  # Pretrained model ID\n    \"seq_len\": 96,\n    \"pred_len\": 24,\n    \"d_ff\": 2048,\n    \"d_model\": 512,\n    \"top_k\": 2,                 # Number of experts to activate\n    \"n_heads\": 8,\n    \"enc_in\": 7,\n}\n</code></pre> <p>Usage Tips: - No training required - use immediately - First run downloads ~200MB model - Designed for forecasting at scale - Good for quick prototyping</p> <p>Example:</p> <pre><code>from liulian.models.torch.timemoe import TimeMoEAdapter\n\nmodel = TimeMoEAdapter(config)\n# First run: Downloads TimeMoE-50M (~200MB)\noutputs = model.run(inputs)  # Zero-shot prediction\n</code></pre>"},{"location":"models/torch_models/#input-data-format","title":"Input Data Format","text":"<p>All models expect NumPy arrays as input. The adapter handles conversion to PyTorch tensors automatically.</p>"},{"location":"models/torch_models/#forecasting-task","title":"Forecasting Task","text":"<pre><code>inputs = {\n    \"x_enc\": np.ndarray,       # (batch, seq_len, enc_in) - Historical data\n    \"x_mark_enc\": np.ndarray,  # (batch, seq_len, 4) - Time features (optional)\n    \"x_dec\": np.ndarray,       # (batch, label_len+pred_len, dec_in) - Decoder input\n    \"x_mark_dec\": np.ndarray,  # (batch, label_len+pred_len, 4) - Decoder time features\n}\n</code></pre>"},{"location":"models/torch_models/#imputation-task","title":"Imputation Task","text":"<pre><code>inputs = {\n    \"x_enc\": np.ndarray,       # (batch, seq_len, enc_in) - Data with missing values\n    \"x_mark_enc\": np.ndarray,  # (batch, seq_len, 4) - Time features\n    \"mask\": np.ndarray,        # (batch, seq_len, enc_in) - Missing value mask\n}\n</code></pre>"},{"location":"models/torch_models/#anomaly-detection-task","title":"Anomaly Detection Task","text":"<pre><code>inputs = {\n    \"x_enc\": np.ndarray,       # (batch, seq_len, enc_in) - Data to check\n}\n</code></pre>"},{"location":"models/torch_models/#classification-task","title":"Classification Task","text":"<pre><code>inputs = {\n    \"x_enc\": np.ndarray,       # (batch, seq_len, enc_in) - Input sequence\n    \"x_mark_enc\": np.ndarray,  # (batch, seq_len, 4) - Time features\n}\n</code></pre>"},{"location":"models/torch_models/#time-features-x_mark_","title":"Time Features (<code>x_mark_*</code>)","text":"<p>Common time features (4 dimensions): - Month of year (normalized) - Day of month (normalized) - Day of week (normalized) - Hour of day (normalized)</p>"},{"location":"models/torch_models/#model-selection-guide","title":"Model Selection Guide","text":""},{"location":"models/torch_models/#by-performance-priority","title":"By Performance Priority","text":"<ol> <li>PatchTST - Best overall accuracy</li> <li>Autoformer - Best for periodic data</li> <li>iTransformer - Best for multivariate</li> <li>Informer - Good long-sequence performance</li> <li>TimeLLM - Novel LLM approach</li> <li>DLinear - Fast baseline</li> <li>TimeMoE - Zero-shot convenience</li> </ol>"},{"location":"models/torch_models/#by-speed-priority","title":"By Speed Priority","text":"<ol> <li>DLinear - Fastest (linear layers only)</li> <li>Informer - Efficient O(L log L)</li> <li>Autoformer - Efficient O(L log L)</li> <li>PatchTST - Moderate (depends on num_patches)</li> <li>iTransformer - Moderate (depends on num_variates)</li> <li>TimeMoE - Moderate</li> <li>TimeLLM - Slowest (LLM overhead)</li> </ol>"},{"location":"models/torch_models/#by-resource-requirements","title":"By Resource Requirements","text":"<p>Lightweight (&lt; 4GB GPU): - DLinear - Informer (with distillation) - Autoformer</p> <p>Moderate (4-8GB GPU): - PatchTST - iTransformer - TimeMoE - TimeLLM (with BERT/GPT2)</p> <p>Heavy (&gt; 8GB GPU): - TimeLLM (with LLAMA)</p>"},{"location":"models/torch_models/#by-data-type","title":"By Data Type","text":"<p>Univariate Time Series: - DLinear (baseline) - PatchTST (best accuracy) - Informer (long sequences)</p> <p>Multivariate with Correlations: - iTransformer (best for inter-variate dependencies) - PatchTST (channel-independent) - Autoformer (decomposition)</p> <p>Periodic/Seasonal Data: - Autoformer (explicitly models seasonality) - PatchTST (strong overall)</p> <p>Long Sequences (&gt; 500 steps): - Informer (ProbSparse attention) - Autoformer (AutoCorrelation) - PatchTST (patch-based)</p>"},{"location":"models/torch_models/#advanced-usage","title":"Advanced Usage","text":""},{"location":"models/torch_models/#model-persistence","title":"Model Persistence","text":"<pre><code># Save model\nmodel.save(\"path/to/model.pth\")\n\n# Load model\nloaded_model = DLinearAdapter(config)\nloaded_model.load(\"path/to/model.pth\")\n</code></pre>"},{"location":"models/torch_models/#gpucpu-control","title":"GPU/CPU Control","text":"<p>Models automatically use CUDA if available. To force CPU:</p> <pre><code>import torch\n\n# Before creating model\ntorch.cuda.is_available = lambda: False\n\nmodel = DLinearAdapter(config)\n</code></pre>"},{"location":"models/torch_models/#batch-processing","title":"Batch Processing","text":"<pre><code># Process multiple batches\nbatch_size = 32\nfor batch in dataloader:\n    inputs = {\n        \"x_enc\": batch[\"historical\"],\n        # ... other inputs\n    }\n    outputs = model.run(inputs)\n    predictions = outputs[\"predictions\"]\n</code></pre>"},{"location":"models/torch_models/#troubleshooting","title":"Troubleshooting","text":""},{"location":"models/torch_models/#import-errors","title":"Import Errors","text":"<p>Problem: <code>ModuleNotFoundError: No module named 'torch'</code></p> <p>Solution: <pre><code>pip install -e \".[torch-models]\"\n</code></pre></p> <p>Problem: <code>ModuleNotFoundError: No module named 'transformers'</code></p> <p>Solution: <pre><code>pip install -e \".[torch-models-full]\"\n</code></pre></p>"},{"location":"models/torch_models/#memory-errors","title":"Memory Errors","text":"<p>Problem: CUDA out of memory</p> <p>Solutions: 1. Reduce batch size 2. Use smaller model (<code>d_model</code>, <code>d_ff</code>, <code>e_layers</code>) 3. Enable distillation (Informer) 4. Use CPU (slower but no memory limit)</p>"},{"location":"models/torch_models/#download-failures","title":"Download Failures","text":"<p>Problem: TimeLLM/TimeMoE download fails</p> <p>Solutions: 1. Check internet connection 2. Check disk space 3. Clear HuggingFace cache: <code>rm -rf ~/.cache/huggingface/</code> 4. Try different mirror (set <code>HF_ENDPOINT</code> environment variable)</p>"},{"location":"models/torch_models/#performance-issues","title":"Performance Issues","text":"<p>Problem: Model is too slow</p> <p>Solutions: 1. Use simpler model (DLinear) 2. Reduce sequence length 3. Increase patch length (PatchTST) 4. Enable distillation (Informer) 5. Use GPU instead of CPU</p>"},{"location":"models/torch_models/#citation","title":"Citation","text":"<p>If you use these models in your research, please cite the original papers:</p> <pre><code>@inproceedings{dlinear2023,\n  title={Are Transformers Effective for Time Series Forecasting?},\n  author={Zeng, Ailing and Chen, Muxi and Zhang, Lei and Xu, Qiang},\n  booktitle={AAAI},\n  year={2023}\n}\n\n@inproceedings{informer2021,\n  title={Informer: Beyond Efficient Transformer for Long Sequence Time-Series Forecasting},\n  author={Zhou, Haoyi and Zhang, Shanghang and Peng, Jieqi and Zhang, Shuai and Li, Jianxin and Xiong, Hui and Zhang, Wancai},\n  booktitle={AAAI},\n  year={2021}\n}\n\n@inproceedings{autoformer2021,\n  title={Autoformer: Decomposition Transformers with Auto-Correlation for Long-Term Series Forecasting},\n  author={Wu, Haixu and Xu, Jiehui and Wang, Jianmin and Long, Mingsheng},\n  booktitle={NeurIPS},\n  year={2021}\n}\n\n@inproceedings{patchtst2023,\n  title={A Time Series is Worth 64 Words: Long-term Forecasting with Transformers},\n  author={Nie, Yuqi and Nguyen, Nam H and Sinthong, Phanwadee and Kalagnanam, Jayant},\n  booktitle={ICLR},\n  year={2023}\n}\n\n@inproceedings{itransformer2024,\n  title={iTransformer: Inverted Transformers Are Effective for Time Series Forecasting},\n  author={Liu, Yong and Hu, Tengge and Zhang, Haoran and Wu, Haixu and Wang, Shiyu and Ma, Lintao and Long, Mingsheng},\n  booktitle={ICLR},\n  year={2024}\n}\n\n@inproceedings{timellm2024,\n  title={Time-LLM: Time Series Forecasting by Reprogramming Large Language Models},\n  author={Jin, Ming and Wang, Shiyu and Ma, Lintao and Chu, Zhixuan and Zhang, James Y and Shi, Xiaoming and Chen, Pin-Yu and Liang, Yuxuan and Li, Yuan-Fang and Pan, Shirui and Wen, Qingsong},\n  booktitle={ICLR},\n  year={2024}\n}\n\n@article{timemoe2024,\n  title={TimeMoE: Billions of Time Series Now Trainable on a Single GPU},\n  author={Shi, Xiaoming and others},\n  journal={arXiv preprint arXiv:2409.16040},\n  year={2024}\n}\n</code></pre>"},{"location":"models/torch_models/#references","title":"References","text":"<ul> <li>Time-Series-Library: https://github.com/thuml/Time-Series-Library</li> <li>Time-LLM: https://github.com/KimMeen/Time-LLM</li> <li>liulian Project: [Link to project repository]</li> <li>Adaptation Report: See <code>artifacts/adaptations/adapt_20260209_155126/report.md</code></li> </ul>"},{"location":"models/torch_models/#support","title":"Support","text":"<p>For issues specific to: - Model implementation: Check traceability docs in <code>artifacts/adaptations/adapt_20260209_155126/traceability/</code> - liulian integration: Check liulian documentation - Original algorithms: Refer to original repositories and papers</p> <p>Last Updated: 2026-02-09 Adaptation ID: adapt_20260209_155126</p>"}]}